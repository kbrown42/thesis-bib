Classic, or template based, NLG systems contain 3 components linked together in a pipeline.  Those stage  In this work, the authors have created a hybrid approach that combines the classic template based approach with statistical learning into a single step by extracting and ranking templates from a historical corpus focused on a single domain.  In this work the authors used data from weather and biography. Their results show that this hybrid approach can be used to generate well formed natural language with a high amount of syntactic variation provided there is sufficient historical data to learn from.  Moreover, by not relying on hand engineered rules-based document planning which is typical in classical approaches the development time needed to adapt the technique to new domains is significantly reduced from months to perhaps only days or weeks.
\\
\\
The results show that this system creates acceptable documents in terms of fluency and understandability.  The system was evaluated using domain experts and non-domain experts as well as a metric the authors created based on syntactic variability.  They find that an overly high syntactic variability score can lead to overly complicated sentences.  In the conclusion, they claim that the majority of their time was spent creating entity taggers for the weather domain corpus.  This could be good news for us if there are as I suspect existing entity taggers for the medical domain.  If not, this is a great opportunity for a contribution on our part.
\\
\\
I think we should consider using this approach as a baseline for our note generation.  It seems effective, not overly complicated, and has pieces that we can swap out and experiment on.  For instance, sentence representation, template ranking and selection, etc.  The MIMIC-III dataset is a promising proving ground for this I believe.
From a high level, the hybrid NLG approach has four phases in document creation: preprocessing, conceptual unit creation, collecting statistics, and building the template ranking model.
\\
\\
The preprocessing stage aims to uncover the underlying semantic structure of the corpus and then to use that structure as a way to create and rank sentence templates.  They begin by segmenting documents into sentences and representing them using Discourse Representation Structure which contains semantic predicates and named entity tags.  Semantic predicates include items such as EVENT, DATE, INSTITUTION, etc. The authors developed their own named-entity tagger for the weather domain.  (Note: we will have to develop entity tags for psych domain unless something else currently exists.)  (Note: the DRS representation of a sentence is unfamiliar to me at this point.  Effective sentence representation for this type of NLG system seems like a good research topic for us to explore). The preprocessing stage outputs a template bank and predicate information for each template in the corpus.
\\
\\
The next stage is clustering templates into conceptual units.  The authors used a k-means clustering algorithm in which they used the semantic predicate information for each template to cluster them together.
Corpus statistics are computed after clustering and these statistics are used in the final stage which is building the template ranking model.  The statistics collected are hand engineered (Note: another clear avenue for research is creating a template ranking algorithm that doesn’t rely on hand engineered features).
\\
\\
The crux of the algorithm is the template ranking model which attempts to rank a set of templates for a given position in the document (sentence 1, sentence 2, …). Training data is extracted from a document in the training set.  The task is to learn the ranks of all templates for all clusters.  First, templates that have entity tags not present in the training data are removed.  Then, the top ten templates are chosen based on Levenshtein edit distance for each position.  That is to say, for the first sentence in the training document the top ten closest templates based on edit distance are chosen.  The same is repeated for each following sentence in the training document.  Another set of hand engineered features are generated for each template and are fed into a linear SVM for ranking.  The authors state that the cost is set to total queries.  The meaning of this is unclear in the paper.
\\
\\
Generation begins with again filtering out templates that have entity tags not present in the input data (Note: the data representation here is unclear but may be present in one of the cited references. Either way, it is something we will have to sort out ourselves).  Templates are scored and ranked using the model constructed from the SVM.  The top ranked template is chosen and the entities replaced with fields from the input data.  This is repeated until the number of sentences reaches the minimum found in the domain and continues until all the input data is placed in a template.  The max number of sentences is based off historical corpus statistics. The authors note that before generating the next sentence, the algorithm must decide whether to remove the already used entities from the data or to keep them for future sentence generation.  Keeping some entities for future sentences can be useful for clarification and coherence.  They state that their system has parameters to control this but do not elaborate further.